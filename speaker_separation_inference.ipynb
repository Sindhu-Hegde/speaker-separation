{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speaker_separation_inference.pynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPX+z5aicNrORlr2ZUhsBo1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sindhu-Hegde/speaker-separation/blob/main/speaker_separation_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM9mBGFBSm17",
        "outputId": "198b1e0c-4851-4b13-929a-f94ca77f14de"
      },
      "source": [
        "!git clone https://github.com/Sindhu-Hegde/speaker-separation"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'speaker-separation'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 131 (delta 16), reused 114 (delta 10), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (131/131), 14.81 MiB | 27.57 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVpbYjZ_Srf_",
        "outputId": "94548798-f093-4068-adc6-e05a4f6a8232"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m47CQMMES0ww",
        "outputId": "2f75a945-7c07-4ee8-bed5-6af1eab85b04"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1B1HVaWZS8OhbZoTY8XPWS8dizONj2aGm -O \"speaker-separation/model_vox.pt\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1B1HVaWZS8OhbZoTY8XPWS8dizONj2aGm\n",
            "To: /content/speaker-separation/model_vox.pt\n",
            "1.33GB [00:12, 104MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsmzIlKuVQ0h",
        "outputId": "2dfb00cf-56d8-42ae-ce04-af4ec7528193"
      },
      "source": [
        "!cd speaker_separation && pip install -r requirements.txt"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting absl-py==0.13.0\n",
            "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 132 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.8.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: audioread==2.1.9 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.1.9)\n",
            "Requirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (4.2.2)\n",
            "Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (2021.5.30)\n",
            "Requirement already satisfied: cffi==1.14.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.14.6)\n",
            "Requirement already satisfied: charset-normalizer==2.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2.0.4)\n",
            "Requirement already satisfied: clang==5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (5.0)\n",
            "Collecting decorator==5.0.9\n",
            "  Downloading decorator-5.0.9-py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: flatbuffers==1.12 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (1.12)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (0.4.0)\n",
            "Collecting google-auth==1.35.0\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (0.4.5)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (0.2.0)\n",
            "Requirement already satisfied: grpcio==1.39.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (1.39.0)\n",
            "Requirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (3.1.0)\n",
            "Collecting idna==3.2\n",
            "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata==4.6.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (4.6.4)\n",
            "Requirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (1.0.1)\n",
            "Requirement already satisfied: keras==2.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (2.6.0)\n",
            "Collecting Keras-Applications==1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 25)) (1.1.2)\n",
            "Requirement already satisfied: librosa==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (0.8.1)\n",
            "Collecting llvmlite==0.36.0\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 76 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Markdown==3.3.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 28)) (3.3.4)\n",
            "Collecting mock==4.0.3\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting numba==0.53.1\n",
            "  Downloading numba-0.53.1-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 31)) (1.19.5)\n",
            "Requirement already satisfied: oauthlib==3.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 32)) (3.1.1)\n",
            "Collecting opencv-python==4.5.3.56\n",
            "  Downloading opencv_python-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.9 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 34)) (3.3.0)\n",
            "Requirement already satisfied: packaging==21.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 35)) (21.0)\n",
            "Requirement already satisfied: pooch==1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 36)) (1.4.0)\n",
            "Requirement already satisfied: protobuf==3.17.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 37)) (3.17.3)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 38)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 39)) (0.2.8)\n",
            "Requirement already satisfied: pycparser==2.20 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 40)) (2.20)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 41)) (2.4.7)\n",
            "Collecting requests==2.26.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 678 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 43)) (1.3.0)\n",
            "Requirement already satisfied: resampy==0.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 44)) (0.2.2)\n",
            "Requirement already satisfied: rsa==4.7.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 45)) (4.7.2)\n",
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.6 MB/s \n",
            "\u001b[?25hCollecting scipy==1.7.1\n",
            "  Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 46 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 48)) (1.15.0)\n",
            "Requirement already satisfied: SoundFile==0.10.3.post1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 49)) (0.10.3.post1)\n",
            "Collecting tensorboard==1.13.1\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard-data-server==0.6.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 51)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 52)) (1.8.0)\n",
            "Collecting tensorflow==1.13.1\n",
            "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6 MB 19 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 55)) (1.1.0)\n",
            "Collecting threadpoolctl==2.2.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 57)) (1.9.0+cu102)\n",
            "Collecting tqdm==4.62.1\n",
            "  Downloading tqdm-4.62.1-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 59)) (3.7.4.3)\n",
            "Collecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 46.6 MB/s \n",
            "\u001b[?25hCollecting Werkzeug==2.0.1\n",
            "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 62)) (1.12.1)\n",
            "Requirement already satisfied: zipp==3.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 63)) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse==1.6.3->-r requirements.txt (line 4)) (0.37.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.35.0->-r requirements.txt (line 15)) (57.4.0)\n",
            "Installing collected packages: urllib3, llvmlite, idna, Werkzeug, threadpoolctl, scipy, requests, numba, mock, absl-py, tensorflow-estimator, tensorboard, scikit-learn, Keras-Applications, google-auth, decorator, tqdm, tensorflow, opencv-python\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.34.0\n",
            "    Uninstalling google-auth-1.34.0:\n",
            "      Successfully uninstalled google-auth-1.34.0\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.2.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.13.0 which is incompatible.\n",
            "moviepy 0.2.3.5 requires decorator<5.0,>=4.0.2, but you have decorator 5.0.9 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Keras-Applications-1.0.8 Werkzeug-2.0.1 absl-py-0.13.0 decorator-5.0.9 google-auth-1.35.0 idna-3.2 llvmlite-0.36.0 mock-4.0.3 numba-0.53.1 opencv-python-4.5.3.56 requests-2.26.0 scikit-learn-0.24.2 scipy-1.7.1 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 threadpoolctl-2.2.0 tqdm-4.62.1 urllib3-1.26.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVlL8Uf8fz7Q"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wiEYYbScV3mH",
        "outputId": "52d078d0-a43f-48d6-b570-4eac721295c2"
      },
      "source": [
        "# !pip uninstall tensorflow\n",
        "# !pip install tensorflow==1.13.1\n",
        "\n",
        "# !pip uninstall librosa\n",
        "# !pip install librosa==0.6.0\n",
        "\n",
        "!pip uninstall numba\n",
        "!pip install numba==0.48.0\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: librosa 0.7.0\n",
            "Uninstalling librosa-0.7.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/librosa-0.7.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/librosa/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/librosa/__pycache__/filters.__window_ss_fill-1156.py37m.1.nbc\n",
            "    /usr/local/lib/python3.7/dist-packages/librosa/__pycache__/filters.__window_ss_fill-1156.py37m.nbi\n",
            "    /usr/local/lib/python3.7/dist-packages/librosa/core/__pycache__/spectrum.__overlap_add-441.py37m.1.nbc\n",
            "    /usr/local/lib/python3.7/dist-packages/librosa/core/__pycache__/spectrum.__overlap_add-441.py37m.nbi\n",
            "Proceed (y/n)? \u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Collecting librosa==0.6.0\n",
            "  Using cached librosa-0.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (5.0.9)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (0.2.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (2.1.9)\n",
            "Requirement already satisfied: joblib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (1.0.1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (1.7.1)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.0->librosa==0.6.0) (0.53.1)\n",
            "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy>=0.2.0->librosa==0.6.0) (0.36.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy>=0.2.0->librosa==0.6.0) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.6.0) (2.2.0)\n",
            "Installing collected packages: librosa\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.7.0\n",
            "    Uninstalling librosa-0.7.0:\n",
            "      Successfully uninstalled librosa-0.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.5 requires librosa>=0.7.2, but you have librosa 0.6.0 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\n",
            "Successfully installed librosa-0.6.0\n",
            "Found existing installation: numba 0.53.1\n",
            "Uninstalling numba-0.53.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/numba\n",
            "    /usr/local/bin/pycc\n",
            "    /usr/local/lib/python3.7/dist-packages/numba-0.53.1.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/numba/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled numba-0.53.1\n",
            "Collecting numba==0.48.0\n",
            "  Downloading numba-0.48.0-1-cp37-cp37m-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.6 MB/s \n",
            "\u001b[?25hCollecting llvmlite<0.32.0,>=0.31.0dev0\n",
            "  Downloading llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.48.0) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba==0.48.0) (1.19.5)\n",
            "Installing collected packages: llvmlite, numba\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.36.0\n",
            "    Uninstalling llvmlite-0.36.0:\n",
            "      Successfully uninstalled llvmlite-0.36.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.5 requires librosa>=0.7.2, but you have librosa 0.6.0 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\n",
            "Successfully installed llvmlite-0.31.0 numba-0.48.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "llvmlite",
                  "numba"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY8s3cE_W3nd"
      },
      "source": [
        "**Import Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ooYY3cyS7ES",
        "outputId": "a59da874-7450-4aff-afa8-7a5bd2b27ec6"
      },
      "source": [
        "from os import listdir, path\n",
        "import numpy as np\n",
        "import speaker_separation\n",
        "import scipy, cv2, os, argparse, speaker_separation.audio\n",
        "import subprocess\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "import speaker_separation.audio.audio_utils as audio\n",
        "from speaker_separation.model import *\n",
        "import speaker_separation.audio.hparams as hp \n",
        "import torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cymNqCaHYyvY",
        "outputId": "a78af606-daac-4f7b-ee62-30d13cade112"
      },
      "source": [
        "# !pip install librosa\n",
        "print(librosa.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_ZbRPRzW__d"
      },
      "source": [
        "**Preprocessing Video File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXcJN2SSXC4l"
      },
      "source": [
        "# Extracts frames from the video\n",
        "def get_frames(file, mask):\n",
        "\n",
        "    video_stream = cv2.VideoCapture(file)\n",
        "\n",
        "    frames = []\n",
        "    while 1:\n",
        "        still_reading, frame = video_stream.read()\n",
        "\n",
        "        if not still_reading:\n",
        "            video_stream.release()\n",
        "            break\n",
        "\n",
        "        # Mask out the specified regions\n",
        "        if mask == 'r':\n",
        "            index = frame.shape[1]//2\n",
        "            frame = frame[:, :index]\n",
        "        elif mask == 'l':\n",
        "            index = frame.shape[1]//2\n",
        "            frame = frame[:, index:]\n",
        "\n",
        "        frames.append(frame)\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Function to obtain the window of images\n",
        "def get_window_images(window_images):\n",
        "    window = []\n",
        "    for img in window_images:\n",
        "        if img is None:\n",
        "            raise FileNotFoundError('Missing frames!')\n",
        "\n",
        "        img = cv2.resize(img, (hp.hparams.img_size, hp.hparams.img_size))\t\t# 3x96x96\n",
        "        window.append(img)\n",
        "\n",
        "    x_image = np.asarray(window) / 255. \n",
        "\n",
        "    return x_image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNoXZ-EhXhnH"
      },
      "source": [
        "**Preprocessing Audio**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0vZ_OfrXFDT"
      },
      "source": [
        "# Functon to load the wav file\n",
        "def load_wav(input_file):\n",
        "\n",
        "    wav_file  = 'tmp.wav';\n",
        "\n",
        "    subprocess.call('ffmpeg -hide_banner -loglevel panic -threads 1 -y -i %s -async 1 -ac 1 -vn \\\n",
        "                    -acodec pcm_s16le -ar 16000 %s' % (input_file, wav_file), shell=True)\n",
        "\n",
        "    wav = audio.load_wav(wav_file, 16000)\n",
        "\n",
        "    os.remove(\"tmp.wav\")\n",
        "\n",
        "    return wav\n",
        "\n",
        "\n",
        "# Function to extract the spectrogram from the wav file\n",
        "def get_spec(wav):\n",
        "\n",
        "    # Extract the STFT\n",
        "    stft = librosa.stft(y=wav, n_fft=hp.hparams.n_fft, hop_length=hp.hparams.hop_size, win_length=hp.hparams.win_size).T\n",
        "    stft = stft[:-1]\n",
        "    # print(\"STFT: \", stft.shape)                                       # 100x257\n",
        "\n",
        "    # Decompose STFT into magnitude and phase components\n",
        "    mag = np.abs(stft)\n",
        "    mag = audio.db_from_amp(mag)\n",
        "    phase = audio.angle(stft)\n",
        "\n",
        "    # Normalize the magnitude and phase components\n",
        "    norm_mag = audio.normalize_mag(mag)\n",
        "    norm_phase = audio.normalize_phase(phase)\n",
        "\n",
        "    # Concatenate the magnitude and phase components\n",
        "    spec = np.concatenate((norm_mag, norm_phase), axis=1)               # 100x514\n",
        "\n",
        "    return spec\n",
        "\n",
        "\n",
        "# Function to segment the spectrograms\n",
        "def get_window_spec(spec_ip, idx):\n",
        "\n",
        "    frame_num = idx\n",
        "    start_idx = int((hp.hparams.spec_step_size / hp.hparams.fps) * frame_num)\n",
        "    end_idx = start_idx+hp.hparams.spec_step_size\n",
        "\n",
        "    spec_window = spec_ip[start_idx:end_idx, :]                        # 100x514\n",
        "\n",
        "    return spec_window"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMPhv9y6Xp4c"
      },
      "source": [
        "**Speaker dependent audio and video generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWEmUbrJXIGV"
      },
      "source": [
        "# Function to reconstruct the audio and generate the output video \n",
        "def generate_video(mag, phase, input_file, result_dir):\n",
        "\n",
        "    denorm_mag = audio.unnormalize_mag(mag)\n",
        "    denorm_phase = audio.unnormalize_phase(phase)\n",
        "    recon_mag = audio.amp_from_db(denorm_mag)\n",
        "    complex_arr = audio.make_complex(recon_mag, denorm_phase)\n",
        "    wav = librosa.istft(complex_arr, hop_length=hp.hparams.hop_size, \\\n",
        "                        win_length=hp.hparams.win_size)\n",
        "    print(\"Generated wav: \", wav.shape)\n",
        "\n",
        "\n",
        "    # Create the folder to save the results\n",
        "    if not os.path.exists(result_dir):\n",
        "        os.makedirs(result_dir)\n",
        "\n",
        "    # Save the wav file\n",
        "    audio_output = os.path.join(result_dir, 'pred_'+input_file.rsplit('/')[-1].split('.')[0] + '.wav')\n",
        "    librosa.output.write_wav(audio_output, wav, 16000)\n",
        "\n",
        "    # Save the video output file\n",
        "    no_sound_video = os.path.join(result_dir, input_file.rsplit('/')[-1].split('.')[0] + '_nosouund.mp4')\n",
        "    subprocess.call('ffmpeg -hide_banner -loglevel panic -i %s -c copy -an -strict -2 %s' % (input_file, no_sound_video), shell=True)\n",
        "\n",
        "    video_output_mp4 = os.path.join(result_dir, 'pred_'+input_file.rsplit('/')[-1].split('.')[0] + '.mp4')\n",
        "    if os.path.exists(video_output_mp4):\n",
        "        os.remove(video_output_mp4)\n",
        "\n",
        "    subprocess.call('ffmpeg -hide_banner -loglevel panic -y -i %s -i %s -strict -2 -q:v 1 %s' % \n",
        "                    (audio_output, no_sound_video, video_output_mp4), shell=True)\n",
        "\n",
        "    os.remove(no_sound_video)\n",
        "\n",
        "    print(\"Successfully generated the output video:\", video_output_mp4)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAFvrLnhXtdO"
      },
      "source": [
        "**Load saved checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeBuvqBnXJuJ"
      },
      "source": [
        "# Function to load the model\n",
        "def load_model(checkpoint):\n",
        "\n",
        "    model = Model()\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        checkpoint = torch.load(checkpoint, map_location='cpu')\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint)\n",
        "\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        ckpt = {}\n",
        "        for key in checkpoint['model_state_dict'].keys():\n",
        "            k = key.split('module.', 1)[1]\n",
        "            ckpt[k] = checkpoint['model_state_dict'][key]\n",
        "        model.load_state_dict(ckpt)\t\n",
        "    # model = model.to(device)\n",
        "\n",
        "#     print(\"Loaded model from: \", checkpoint)\n",
        "\n",
        "    return model.eval()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EDfpCQFXwpG"
      },
      "source": [
        "**Generate predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpSvoHh6XLMV"
      },
      "source": [
        "# Function to obtain the predictions\n",
        "def predict(input_file, num_frames, mask, checkpoint, result_dir):\n",
        "\n",
        "    # Check the input video\n",
        "    video_formats = ['mp4', 'avi', 'mkv']\n",
        "    if input_file.rsplit('/')[-1].rsplit('.', 1)[1] not in video_formats:\n",
        "        print(\"Oops! Invalid input. Please try again by providing the appropriate video input.\")\n",
        "        exit(0)\n",
        "\n",
        "    # Extract the frames from the given input video\n",
        "    faces = get_frames(input_file, mask)\n",
        "    total_frames = len(faces)\n",
        "\n",
        "    print(f'No of frames: {len(faces)}')\n",
        "    if len(faces) < num_frames: \n",
        "        print(\"No of frames is less than {}!\".format(num_frames))\n",
        "        return\n",
        "    print(\"Total no of frames = \", total_frames)\n",
        "\n",
        "    # Obtain a window for frames\n",
        "    id_windows = [range(i, i + num_frames) for i in range(0, total_frames, \n",
        "                num_frames - hp.hparams.overlap) if (i + num_frames <= total_frames)]\n",
        "    print(\"ID windows: \", id_windows)\n",
        "\n",
        "    all_images = [[faces[i] for i in window] for window in id_windows]\n",
        "    print(\"All images: \", len(all_images))\n",
        "\n",
        "    inp_wav = load_wav(input_file)\n",
        "    spec_ip = get_spec(inp_wav)\n",
        "    print(\"Noisy spec inp: \", spec_ip.shape)\n",
        "\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model(checkpoint)\n",
        "\n",
        "\n",
        "    for i, window_images in enumerate(tqdm(all_images)):\n",
        "\n",
        "        images = get_window_images(window_images)\n",
        "\n",
        "        if(images.shape[0] != num_frames):\n",
        "            continue\n",
        "        image_batch = np.expand_dims(images, axis=0)\t\t\t#1x25x15x48x96\n",
        "\n",
        "        # Get the corresponding input noisy melspectrograms\n",
        "        idx = id_windows[i][0]\n",
        "        spec_window = get_window_spec(spec_ip, idx)\n",
        "\n",
        "        if(spec_window.shape[0] != hp.hparams.spec_step_size):\n",
        "            continue\n",
        "        spec_batch = np.expand_dims(np.array(spec_window), axis=0)\n",
        "\n",
        "        x_mag = torch.FloatTensor(spec_batch)[..., :257]\n",
        "        x_phase = torch.FloatTensor(spec_batch)[..., 257:]\n",
        "        x_image = torch.FloatTensor(image_batch)\n",
        "\n",
        "        # Predict the spectrograms for the corresponding window\n",
        "        with torch.no_grad():\n",
        "            pred_mag, pred_phase = model(x_mag, x_phase, x_image)\n",
        "\n",
        "\n",
        "        pred_mag = pred_mag.cpu().numpy()\n",
        "        pred_mag = np.squeeze(pred_mag, axis=0).T\n",
        "\n",
        "        pred_phase = pred_phase.cpu().numpy()\n",
        "        pred_phase = np.squeeze(pred_phase, axis=0).T\n",
        "\n",
        "\n",
        "        # Concatenate the melspectrogram windows to generate the complete spectrogram\t\n",
        "        if i == 0:\n",
        "            generated_mag = pred_mag[:, :80]\n",
        "            generated_phase = pred_phase[:, :80]\n",
        "        else:\n",
        "            generated_mag = np.concatenate((generated_mag, pred_mag[:, :80]), axis=1)\n",
        "            generated_phase = np.concatenate((generated_phase, pred_phase[:, :80]), axis=1)\n",
        "\n",
        "\n",
        "    print(\"Output mag: \", generated_mag.shape)\n",
        "    print(\"Output phase: \", generated_phase.shape)\n",
        "\n",
        "    # Reconstruct the audio and generate the output video\n",
        "    generate_video(generated_mag, generated_phase, input_file, result_dir)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCOC2cDVX0RF"
      },
      "source": [
        "**Run Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le7pTfeOXNF9",
        "outputId": "ddb75eef-4025-4d03-9431-773bd74384b1"
      },
      "source": [
        "checkpoint = 'speaker_separation/model_vox.pt'\n",
        "input_file = 'speaker_separation/test_inputs/2.mp4'\n",
        "mask = None # 'l' or 'r' is used to specify which speaker to mask\n",
        "result_dir = 'speaker_separation/results1'\n",
        "sampling_rate = 16000 # audio sampling rate\n",
        "fps = 25 # video fps\n",
        "\n",
        "img_size = 96\n",
        "num_frames = 25\n",
        "\n",
        "predict(input_file, num_frames, mask, checkpoint, result_dir)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of frames: 80\n",
            "Total no of frames =  80\n",
            "ID windows:  [range(0, 25), range(20, 45), range(40, 65)]\n",
            "All images:  3\n",
            "Noisy spec inp:  (262, 514)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output mag:  (257, 240)\n",
            "Output phase:  (257, 240)\n",
            "Generated wav:  (38240,)\n",
            "Successfully generated the output video: speaker_separation/results1/pred_2.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}